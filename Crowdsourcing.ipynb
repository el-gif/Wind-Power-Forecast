{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plausibility check of crowdsourced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.stats import ks_2samp\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "json_path = \"E:\\MA_data\\WPPs+production\\WPPs+production_new_complete.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_normalised_productions = []\n",
    "\n",
    "for entry in data:\n",
    "    capacity = entry.get(\"Capacity\")\n",
    "    production_data = entry.get(\"Production\")\n",
    "\n",
    "    for timestamp, value in production_data:\n",
    "        if isinstance(value, (int, float)) and capacity > 0:\n",
    "            all_normalised_productions.append(value / capacity)\n",
    "\n",
    "# In numpy umwandeln\n",
    "arr = np.array(all_normalised_productions)\n",
    "\n",
    "# Erwartungswert und Standardabweichung berechnen\n",
    "mean = arr.mean()\n",
    "std = arr.std()\n",
    "\n",
    "print(\"Original dataset\")\n",
    "print(f\"    number of normalised values: {len(arr)}\")\n",
    "print(f\"    expectation value: {mean:.4f}\")\n",
    "print(f\"    standard deviation: {std:.4f}\")\n",
    "\n",
    "\n",
    "input_dir = \"data/crowdsourced_data/\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        print(f\"Dataset {file}\")\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "        time_series_data = sheets[\"Time Series\"]\n",
    "        capacity = sheets[\"Metadata\"].iloc[0][\"Capacity (MW)\"]\n",
    "        productions = time_series_data[\"Production (MW)\"]\n",
    "        norm_productions_crs = productions / capacity\n",
    "\n",
    "        total = len(norm_productions_crs.dropna())\n",
    "        below_zero_pct = (norm_productions_crs < 0).sum() / total * 100\n",
    "        above_max_pct = (norm_productions_crs > 1.05).sum() / total * 100\n",
    "        std_crs = norm_productions_crs.std()\n",
    "        var_sim = std_crs**2 / std**2 # Variance similarity\n",
    "        mean_crs = norm_productions_crs.mean()\n",
    "        mean_sim = mean_crs / mean # Mean similarity\n",
    "\n",
    "        if below_zero_pct > 0.01:\n",
    "            print(f\"    Discard dataset: {below_zero_pct} % of normalised power values below 0\")\n",
    "\n",
    "        if above_max_pct > 0.05:\n",
    "            print(f\"    Discard dataset: {above_max_pct} % of normalised power values above 1.05\")\n",
    "\n",
    "        if var_sim > 1.5:\n",
    "            print(f\"    Discard dataset: Standard deviation = {std_crs} too high\")\n",
    "\n",
    "        if var_sim < 0.5:\n",
    "            print(f\"    Discard dataset: Standard deviation = {std_crs} too low\")\n",
    "\n",
    "        if mean_sim > 1.5:\n",
    "            print(f\"    Discard dataset: Mean = {mean_crs} too high\")\n",
    "\n",
    "        if mean_sim < 0.5:\n",
    "            print(f\"    Discard dataset: Mean = {mean_crs} too low\")\n",
    "\n",
    "        # Kolmogorov–Smirnov-Test (KS-Test)\n",
    "        statistic, p_value = ks_2samp(all_normalised_productions, norm_productions_crs)\n",
    "        print(f\"statistic: {statistic}, p_value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example crowdsourced data not very representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(all_normalised_productions, bins=100, alpha=0.5, label=\"Reference\", density=True)\n",
    "plt.hist(norm_productions_crs, bins=100, alpha=0.5, label=\"Crowdsourced\", density=True)\n",
    "plt.xlabel(\"Normalised Production\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>(Manually delete crowdsourced data that doesn't seem plausible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load crowdsourced data and fetch according wind data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cdsapi  # for reanalysis data\n",
    "from ecmwfapi import ECMWFService  # for reforecast data\n",
    "\n",
    "input_dir = \"data/crowdsourced_data/\"\n",
    "output_dir = \"data/crowdsourcing_wind\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "        time_series_data = sheets[\"Time Series\"]\n",
    "        metadata = sheets[\"Metadata\"]\n",
    "\n",
    "        timestamp = file.removeprefix(\"time_series_\").removesuffix(\".xlsx\")\n",
    "\n",
    "        time_series_data[\"Date\"] = pd.to_datetime(time_series_data[\"Date\"], errors='coerce')\n",
    "\n",
    "        # Extract unique years, months, days, and hours\n",
    "        years = sorted(set(time_series_data[\"Date\"].dt.year.dropna().astype(str)))\n",
    "        months = sorted(set(time_series_data[\"Date\"].dt.month.dropna().astype(str).str.zfill(2)))\n",
    "        days = sorted(set(time_series_data[\"Date\"].dt.day.dropna().astype(str).str.zfill(2)))\n",
    "        hours = sorted(set(time_series_data[\"Date\"].dt.hour.dropna().astype(str).str.zfill(2)))\n",
    "\n",
    "        # Extract location\n",
    "        lat, lon = metadata.iloc[0][\"Latitude\"], metadata.iloc[0][\"Longitude\"]\n",
    "\n",
    "        # =================== REANALYSIS DATA (ERA5) ===================\n",
    "\n",
    "        # # API key saved in .cdsapirc\n",
    "        # client = cdsapi.Client()\n",
    "        # reanalysis_file = os.path.join(output_dir, f\"reanalysis_{timestamp}.grib\")\n",
    "\n",
    "        # if not os.path.exists(reanalysis_file):  # Skip if already downloaded\n",
    "        #     print(f\"Downloading ERA5 reanalysis data for {timestamp}...\")\n",
    "\n",
    "        #     request = {\n",
    "        #         \"product_type\": \"reanalysis\",\n",
    "        #         \"variable\": [\"100m_u_component_of_wind\", \"100m_v_component_of_wind\"],\n",
    "        #         \"year\": years,\n",
    "        #         \"month\": months,\n",
    "        #         \"day\": days,\n",
    "        #         \"time\": hours,\n",
    "        #         \"format\": \"grib\",\n",
    "        #         \"area\": [lat+1, lon-1, lat-1, lon+1],  # N/W/S/E\n",
    "        #     }\n",
    "\n",
    "        #     client.retrieve(\"reanalysis-era5-single-levels\", request, reanalysis_file)\n",
    "        #     print(f\"Saved reanalysis data: {reanalysis_file}\")\n",
    "        # else:\n",
    "        #     print(f\"Skipping ERA5 reanalysis data for {timestamp}, already exists.\")\n",
    "\n",
    "        # =================== REFORECAST DATA (ECMWF) ===================\n",
    "\n",
    "        # API key under https://api.ecmwf.int/v1/key/, saved in .ecmwfapirc\n",
    "        server = ECMWFService(\"mars\")\n",
    "        reforecast_file = os.path.join(output_dir, f\"reforecast_{timestamp}.grib\")\n",
    "\n",
    "        if not os.path.exists(reforecast_file):  # Skip if already downloaded\n",
    "            print(f\"Downloading ECMWF reforecast data for {timestamp}...\")\n",
    "\n",
    "            server.execute( # ignore efficient MARS request guidelines here, since process should be automated and available timestamps cannot be analysed individually\n",
    "                {\n",
    "                    \"class\": \"od\",\n",
    "                    \"dataset\": \"oper\",\n",
    "                    \"expver\": \"1\",\n",
    "                    \"stream\": \"oper\",\n",
    "                    \"type\": \"fc\",\n",
    "                    \"levtype\": \"sfc\",  # surface-level type\n",
    "                    \"param\": \"246.228/247.228\",  # u100 and v100\n",
    "                    \"date\": f\"{min(years)}-{min(months)}-01/to/{max(years)}-{max(months)}-31\",  # Ensure correct format\n",
    "                    \"time\": [\"00:00\", \"12:00\"],  # Forecast times\n",
    "                    \"step\": [str(i) for i in range(0, 145, 3)],  # Convert to list of strings\n",
    "                    \"grid\": \"0.25/0.25\",\n",
    "                    \"area\": [lat, lon, lat, lon],  # N/W/S/E\n",
    "                    \"format\": \"grib2\",\n",
    "                },\n",
    "                reforecast_file\n",
    "            )\n",
    "            print(f\"Saved reforecast data: {reforecast_file}\")\n",
    "        else:\n",
    "            print(f\"Skipping ECMWF reforecast data for {timestamp}, already exists.\")\n",
    "\n",
    "print(\"All downloads completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build one dictionary for all WPPs (part I), and for each WPP, assign wind data (part II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Define lead times\n",
    "lead_times = list(range(0, 145, 3))\n",
    "\n",
    "# Directories\n",
    "input_dir = \"data/crowdsourced_data\"\n",
    "forecast_dir = \"data/crowdsourcing_wind\"\n",
    "output_dir = \"data/crowdsourcing_wind_production/json_lead_times\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load crowdsourced WPP production data\n",
    "wpp_files = [f for f in os.listdir(input_dir) if f.endswith(\".xlsx\")]\n",
    "wpps = []\n",
    "\n",
    "for file in wpp_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    sheets = pd.read_excel(file_path, sheet_name=[\"Time Series\", \"Metadata\"])\n",
    "    time_series_data = sheets[\"Time Series\"]\n",
    "    metadata = sheets[\"Metadata\"]\n",
    "\n",
    "    time_series_data[\"Date\"] = pd.to_datetime(time_series_data[\"Date\"], errors='coerce')\n",
    "    \n",
    "    upload_time = file.replace('time_series_', '').replace('.xlsx', '')\n",
    "    lat = metadata.iloc[0][\"Latitude\"]\n",
    "    lon = metadata.iloc[0][\"Longitude\"]\n",
    "    turbine_type = metadata.iloc[0][\"Turbine Type\"]\n",
    "    hub_height = metadata.iloc[0][\"Hub Height\"]\n",
    "    comm_year = metadata.iloc[0][\"Commissioning Year\"]\n",
    "    comm_month = metadata.iloc[0][\"Commissioning Month\"]\n",
    "    capacity = metadata.iloc[0][\"Capacity (MW)\"]\n",
    "\n",
    "    # designation of keys just like in existing dataset II!\n",
    "    wpps.append({\n",
    "        \"Upload Time\": upload_time,\n",
    "        \"Latitude\": metadata.iloc[0][\"Latitude\"],\n",
    "        \"Longitude\": metadata.iloc[0][\"Longitude\"],\n",
    "        \"Turbine\": metadata.iloc[0][\"Turbine Type\"],\n",
    "        \"Hub_height\": metadata.iloc[0][\"Hub Height\"],\n",
    "        \"Commissioning_date\": f\"{int(metadata.iloc[0][\"Commissioning Year\"])}/{int(metadata.iloc[0][\"Commissioning Month\"])}\",\n",
    "        \"Capacity\": metadata.iloc[0][\"Capacity (MW)\"],\n",
    "        \"Production\": time_series_data.values.tolist()  # Store entire time series\n",
    "    })\n",
    "\n",
    "grib_files = sorted([f for f in os.listdir(forecast_dir) if f.endswith(\".grib\")])\n",
    "\n",
    "# Process each reforecast file\n",
    "for grib_file in grib_files:\n",
    "    \n",
    "    grib_path = os.path.join(forecast_dir, grib_file)\n",
    "\n",
    "    # Load wind speed forecast data\n",
    "    try:\n",
    "        ds = xr.open_dataset(grib_path, engine=\"cfgrib\", chunks={\"time\": 100})\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {grib_file}: {e} --> skipping\")\n",
    "        continue\n",
    "\n",
    "    upload_time = grib_file.replace('reforecast_', '').replace('.grib', '')\n",
    "\n",
    "    file_path = os.path.join(input_dir, f\"{grib_file.replace('reforecast', 'time_series').replace('grib', 'xlsx')}\")\n",
    "    matching_wpps = [wpp for wpp in wpps if wpp[\"Upload Time\"] == upload_time]\n",
    "    wpp = matching_wpps[0]\n",
    "\n",
    "    lead_time_dicts = {str(lt): {} for lt in lead_times}\n",
    "\n",
    "    # Output JSON file path for this month-year\n",
    "    output_json_file = os.path.join(output_dir, f\"{grib_file.replace('.grib', '_wind.json')}\")\n",
    "\n",
    "    # Check if this month's data has already been processed (only when this cell is restarted before execution has finished)\n",
    "    if os.path.exists(output_json_file):\n",
    "        print(f\"Skipping {output_json_file}, already exists.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {grib_file}...\")\n",
    "\n",
    "    # Extract forecast timestamps and wind components\n",
    "    times = pd.to_datetime(ds[\"valid_time\"].values)\n",
    "    latitudes = ds[\"latitude\"].values\n",
    "    longitudes = ds[\"longitude\"].values\n",
    "    u = ds[\"u100\"].values\n",
    "    v = ds[\"v100\"].values\n",
    "\n",
    "    unique_key = wpp[\"Upload Time\"]  # Unique identifier of a wpp\n",
    "    lon = wpp[\"Longitude\"]\n",
    "    lat = wpp[\"Latitude\"]\n",
    "    time_series = wpp[\"Production\"]\n",
    "\n",
    "    forecast_data = []\n",
    "\n",
    "    for time_series_value in time_series:\n",
    "        timestep = time_series_value[0]\n",
    "        production = time_series_value[1]\n",
    "        for j in range(len(times)):\n",
    "            forecast_times = times[j]\n",
    "            if timestep in forecast_times: # crowdsourced timesteps that are not dividable by 3 (resolution of reforecsts) have to be discarded\n",
    "                forecast_times = pd.DatetimeIndex(forecast_times) # convert from DatetimeArray to DatetimeIndex, because only this one has .get_loc()\n",
    "                time_index = forecast_times.get_loc(timestep)\n",
    "                lead_time = lead_times[time_index]\n",
    "\n",
    "                forecast_u = u[j]\n",
    "                forecast_v = v[j]\n",
    "\n",
    "                wind_speeds = np.sqrt(forecast_u[time_index]**2 + forecast_v[time_index]**2)\n",
    "\n",
    "                if latitudes[0] < latitudes[-1]:\n",
    "                    latitudes = latitudes[::-1]\n",
    "                    wind_speeds = wind_speeds[::-1, :]\n",
    "\n",
    "                interpolator = RegularGridInterpolator(\n",
    "                    (latitudes, longitudes), wind_speeds, method=\"linear\", bounds_error=False, fill_value=None\n",
    "                )\n",
    "\n",
    "                wind_speed_value = interpolator([[lat, lon]])[0]\n",
    "\n",
    "                wind_speed_value = round(wind_speed_value, 2)\n",
    "\n",
    "                forecast_data.append([lead_time, timestep, production, wind_speed_value])\n",
    "\n",
    "    forecast_df = pd.DataFrame(forecast_data, columns=[\"lead_time\", \"forecast_time\", \"production\", \"wind_speed\"])\n",
    "\n",
    "    # Store data in structured dictionary grouped by lead time\n",
    "    for lead_time in lead_times:\n",
    "        lead_time_str = str(lead_time)  # Ensure keys are strings for JSON compatibility\n",
    "\n",
    "        lead_time_mask = forecast_df[\"lead_time\"] == lead_time\n",
    "        lead_time_data = forecast_df[lead_time_mask]\n",
    "\n",
    "        if lead_time_data.empty:\n",
    "            continue\n",
    "\n",
    "        lead_time_dicts[lead_time_str][unique_key] = {\n",
    "            \"Upload_time\": str(wpp[\"Upload Time\"]),\n",
    "            \"Capacity\": float(wpp[\"Capacity\"]),\n",
    "            \"Latitude\": float(wpp[\"Latitude\"]),\n",
    "            \"Longitude\": float(wpp[\"Longitude\"]),\n",
    "            \"Turbine\": str(wpp[\"Turbine\"]),\n",
    "            \"Hub_height\": float(wpp[\"Hub_height\"]),\n",
    "            \"Commissioning_date\": str(wpp[\"Commissioning_date\"]),\n",
    "            \"Time Series\": [\n",
    "                [str(date), float(production), float(wind_speed)]\n",
    "                for _, date, production, wind_speed in lead_time_data[[\"lead_time\", \"forecast_time\", \"production\", \"wind_speed\"]].values.tolist()\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(lead_time_dicts, f, indent=4)\n",
    "\n",
    "    print(f\"Updated JSON file saved: {output_json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Merge all files into one per lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to input JSON files\n",
    "input_json_dir = \"data/crowdsourcing_wind_production/json_lead_times\"\n",
    "\n",
    "# Dictionary to hold all data sorted by lead time\n",
    "lead_time_dicts = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "# Load all JSON files\n",
    "json_files = [f for f in os.listdir(input_json_dir) if f.endswith(\"_wind.json\")]\n",
    "\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(input_json_dir, json_file)\n",
    "    print(f\"Loading {json_file}\")\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Iterate over lead times in the loaded JSON\n",
    "    for lead_time, wpp_data in data.items():\n",
    "        for unique_key, wpp_entry in wpp_data.items():\n",
    "            if unique_key in lead_time_dicts[lead_time]:\n",
    "                # **Merge production data if the WPP already exists**\n",
    "                lead_time_dicts[lead_time][unique_key][\"Time Series\"].extend(wpp_entry[\"Time Series\"])\n",
    "            else:\n",
    "                # **Otherwise, add the entire WPP entry**\n",
    "                lead_time_dicts[lead_time][unique_key] = wpp_entry\n",
    "\n",
    "# Save merged data into separate files per lead time\n",
    "output_dir = \"data/crowdsourcing_wind_production\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for lead_time, wpp_entries in lead_time_dicts.items():\n",
    "    output_file = os.path.join(output_dir, f\"WPPs+production+wind_lead_time_{lead_time}.json\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(wpp_entries, f, indent=4)\n",
    "    print(f\"Saved merged file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Second plausibility test to verify crowdsourced data: old model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "encoders = joblib.load(\"modelC/parameters/encoders.pkl\")\n",
    "input_sizes = joblib.load(\"modelC/parameters/input_sizes.pkl\")\n",
    "scalers = joblib.load(\"modelC/parameters/scalers.pkl\")\n",
    "test_indices = joblib.load(\"modelC/parameters/test_indices.pkl\") # same random_split as during training of the model, so that testing now only is done on unseen data \n",
    "model_state_dicts = torch.load(\"modelC/parameters/models.pth\", weights_only=True)\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "metrics = {}\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # shuffling matters here\n",
    "    data_loader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Testen\n",
    "    print(f\"    Testing\")\n",
    "    model.eval()\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "    test_loss_huber /= len(data_loader)\n",
    "    \n",
    "    metrics[lead_time] = {\n",
    "        \"Huber\": test_loss_huber\n",
    "    }\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>(In case of too high errors with the old model, the data could be discarded as well. However, this needs some closer considerations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extend dataset by verified crowdsourced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Aktueller Zeitstempel\n",
    "current_utc = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Verzeichnisse\n",
    "old_data_path_1 = r\"E:\\MA_data\\WPPs+production+reforecast\"\n",
    "old_data_path_2 = \"data/crowdsourcing_wind_production\"\n",
    "new_data_path = rf\"E:\\MA_data\\WPPs+production+reforecast_crs_{current_utc}\"\n",
    "\n",
    "# Zielverzeichnis erstellen, falls nicht vorhanden\n",
    "os.makedirs(new_data_path, exist_ok=True)\n",
    "\n",
    "# Korrektur: os.listdir (nicht os.listfir)\n",
    "for file in os.listdir(old_data_path_1):\n",
    "    file_path_1 = os.path.join(old_data_path_1, file)\n",
    "    if not os.path.isfile(file_path_1):\n",
    "        continue\n",
    "\n",
    "    # Führt nur `.json`-Dateien weiter aus\n",
    "    if not file.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Extrahiere lead_time\n",
    "    try:\n",
    "        lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "    except ValueError:\n",
    "        print(f\"Dateiname nicht gültig für lead_time: {file}\")\n",
    "        continue\n",
    "\n",
    "    with open(file_path_1, \"r\", encoding=\"utf-8\") as f1:\n",
    "        data1 = json.load(f1)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    # Passende Datei im zweiten Verzeichnis suchen\n",
    "    file2_name = f\"WPPs+production+wind_lead_time_{lead_time}.json\"\n",
    "    file_path_2 = os.path.join(old_data_path_2, file2_name)\n",
    "\n",
    "    # Nur zusammenführen, wenn die zweite Datei auch existiert\n",
    "    if os.path.exists(file_path_2):\n",
    "        with open(file_path_2, \"r\", encoding=\"utf-8\") as f2:\n",
    "            data2 = json.load(f2)\n",
    "\n",
    "        # Beide Dictionaries kombinieren (überschreibt Schlüssel bei Konflikt – ggf. anpassen)\n",
    "        combined_data = {**data1, **data2}\n",
    "    else:\n",
    "        combined_data = data1  # Nur Daten aus Pfad 1\n",
    "\n",
    "    # Datei im Zielverzeichnis speichern\n",
    "    output_path = os.path.join(new_data_path, f\"WPPs+production+wind_lead_time_{lead_time}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(combined_data, out_file, indent=4)\n",
    "\n",
    "    print(f\"Saved combined data to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Retrain from scratch with extended dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import HuberLoss, MSELoss, L1Loss\n",
    "\n",
    "# Lists to store models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "input_sizes = {}\n",
    "metrics = {}\n",
    "\n",
    "# Define MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3366)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "input_dir = r\"E:\\MA_data\\WPPs+production+reforecast_crs_20250328_154208\"\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    if not os.path.isfile(file_path):  # Ensure it's a file (not a folder)\n",
    "        continue\n",
    "\n",
    "    lead_time = int(file.split(\"_\")[-1].replace(\".json\", \"\"))\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        forecast_data = json.load(file)\n",
    "\n",
    "    print(f\"Processing lead time: {lead_time}\")\n",
    "\n",
    "    print(f\"    Data preparation\")\n",
    "\n",
    "    all_turbine_types = []\n",
    "    all_hub_heights = []\n",
    "    all_capacities = []\n",
    "    all_commissioning_dates = []\n",
    "    all_production_data = []\n",
    "\n",
    "    for unique_key, wpp in forecast_data.items():\n",
    "        all_turbine_types.append(str(wpp[\"Turbine\"]))\n",
    "        all_hub_heights.append(wpp[\"Hub_height\"])\n",
    "        all_capacities.append(wpp[\"Capacity\"])\n",
    "        all_commissioning_dates.append(f\"{wpp['Commissioning_date']}/06\" if isinstance(wpp[\"Commissioning_date\"], str) and \"/\" not in wpp[\"Commissioning_date\"] else wpp[\"Commissioning_date\"])\n",
    "        all_production_data.append(wpp[\"Time Series\"])\n",
    "\n",
    "    # One-Hot-Encoding for turbine types\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    turbine_types_onehot = encoder.fit_transform(np.array(all_turbine_types).reshape(-1, 1))\n",
    "\n",
    "    # convert to datetime\n",
    "    standardised_dates = pd.to_datetime(all_commissioning_dates, format='%Y/%m')\n",
    "\n",
    "    # calculate age\n",
    "    ref_date = pd.Timestamp(\"2024-12-01\")\n",
    "    ages = ref_date.year * 12 + ref_date.month - (standardised_dates.year * 12 + standardised_dates.month)\n",
    "\n",
    "    # create combined features and output lists\n",
    "    combined_features_raw = []\n",
    "    output_raw = []\n",
    "    \n",
    "    # convert data in feature arrays\n",
    "    for idx, production_data in enumerate(all_production_data):\n",
    "        num_rows = len(production_data)\n",
    "\n",
    "        # Repetitions for common features\n",
    "        turbine_type_repeated = np.tile(turbine_types_onehot[idx], (num_rows, 1))\n",
    "        hub_height_repeated = np.full((num_rows, 1), float(all_hub_heights[idx]))\n",
    "        age_repeated = np.full((num_rows, 1), ages[idx])\n",
    "\n",
    "        # Extract production values and wind speeds\n",
    "        production_values = np.array([entry[1] for entry in production_data]).reshape(-1, 1) / all_capacities[idx]\n",
    "        wind_speeds = np.array([entry[2] for entry in production_data]).reshape(-1, 1)\n",
    "\n",
    "        # combine all features\n",
    "        combined_chunk = np.hstack((\n",
    "            turbine_type_repeated,\n",
    "            hub_height_repeated,\n",
    "            age_repeated,\n",
    "            wind_speeds\n",
    "        ))\n",
    "\n",
    "        # add the data\n",
    "        combined_features_raw.append(combined_chunk)\n",
    "        output_raw.append(production_values)\n",
    "\n",
    "    # combine all data chunks to one array\n",
    "    combined_features = np.vstack(combined_features_raw)\n",
    "    output = np.vstack(output_raw)\n",
    "\n",
    "    # Interpolate missing values (linear interpolation) in pandas\n",
    "    wind_speed_series = pd.Series(combined_features[:, -1])\n",
    "    wind_speed_series.interpolate(method='linear', inplace=True)\n",
    "    combined_features[:, -1] = wind_speed_series.to_numpy()\n",
    "\n",
    "    # round all values to four decimal places\n",
    "    combined_features = np.round(combined_features, decimals=4)\n",
    "    output = np.round(output, decimals=4)\n",
    "        \n",
    "    # Normalise numerical features\n",
    "    scaler_wind = StandardScaler()\n",
    "    scaler_ages = StandardScaler()\n",
    "    scaler_hub_heights = StandardScaler()\n",
    "\n",
    "    # Skalieren der einzelnen Features\n",
    "    combined_features[:, -1] = scaler_wind.fit_transform(combined_features[:, -1].reshape(-1, 1)).flatten() # scale wind speeds\n",
    "    combined_features[:, -2] = scaler_ages.fit_transform(combined_features[:, -2].reshape(-1, 1)).flatten()  # scale ages\n",
    "    combined_features[:, -3] = scaler_hub_heights.fit_transform(combined_features[:, -3].reshape(-1, 1)).flatten()  # scale hub heights\n",
    "    \n",
    "    # Convert to PyTorch Dataset\n",
    "    dataset = WindPowerDataset(combined_features, output)\n",
    "    \n",
    "    params = {\"batch_size\": 128,\n",
    "              \"lr\": 0.00010155,\n",
    "              \"number_epochs\": 10}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    torch.manual_seed(0)\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # shuffling doesn't matter here, has already taken place during random_split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    # Model setup\n",
    "    input_size = combined_features.shape[1]\n",
    "\n",
    "    # use static instead of dynamic computational graphs\n",
    "    model = torch.jit.script(MLP(input_size=input_size)).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Trainings-Konfiguration\n",
    "    mae_criterion = L1Loss()\n",
    "    mse_criterion = MSELoss()\n",
    "    huber_criterion = HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    # Training\n",
    "    print(f\"    Training\")\n",
    "    for epoch in range(params[\"number_epochs\"]):\n",
    "        print(f\"        Epoch {epoch + 1}/{params['number_epochs']}\")\n",
    "        model.train()\n",
    "        train_loss_mae, train_loss_mse, train_loss_huber = 0, 0, 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate metrics for each criterion\n",
    "            loss_mae = mae_criterion(outputs, batch_y)\n",
    "            loss_mse = mse_criterion(outputs, batch_y)\n",
    "            loss_huber = huber_criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_huber.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics for logging\n",
    "            train_loss_mae += loss_mae.item()\n",
    "            train_loss_mse += loss_mse.item()\n",
    "            train_loss_huber += loss_huber.item()\n",
    "\n",
    "        train_loss_mae /= len(train_loader)\n",
    "        train_loss_mse /= len(train_loader)\n",
    "        train_loss_huber /= len(train_loader)\n",
    "\n",
    "    # Testen\n",
    "    print(f\"    Testing\")\n",
    "    model.eval()\n",
    "\n",
    "    test_loss_mae, test_loss_mse, test_loss_huber = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            \n",
    "            test_loss_mae += mae_criterion(preds, batch_y).item()\n",
    "            test_loss_mse += mse_criterion(preds, batch_y).item()\n",
    "            test_loss_huber += huber_criterion(preds, batch_y).item()\n",
    "\n",
    "    test_loss_mae /= len(test_loader)\n",
    "    test_loss_mse /= len(test_loader)\n",
    "    test_loss_huber /= len(test_loader)\n",
    "    \n",
    "    models[lead_time] = model.state_dict()\n",
    "    \n",
    "    scalers[lead_time] = {\n",
    "        \"winds\": scaler_wind,\n",
    "        \"ages\": scaler_ages,\n",
    "        \"hub_heights\": scaler_hub_heights\n",
    "    }\n",
    "\n",
    "    encoders[lead_time] = encoder\n",
    "\n",
    "    input_sizes[lead_time] = input_size\n",
    "\n",
    "    metrics[lead_time] = {\n",
    "        \"Training\": {\n",
    "            \"Huber\": train_loss_huber,\n",
    "            \"MAE\": train_loss_mae,\n",
    "            \"MSE\":train_loss_mse,\n",
    "            \"RMSE\": np.sqrt(train_loss_mse)\n",
    "        },\n",
    "        \"Testing\": {\n",
    "            \"Huber\": test_loss_huber,\n",
    "            \"MAE\": test_loss_mae,\n",
    "            \"MSE\": test_loss_mse,\n",
    "            \"RMSE\": np.sqrt(test_loss_mse)\n",
    "        },\n",
    "    }\n",
    "\n",
    "current_utc = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder = f\"modelC/parameters_crs_{current_utc}\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Save all parameters\n",
    "torch.save(models, os.path.join(folder, \"models.pth\"))\n",
    "joblib.dump(scalers, os.path.join(folder, \"scalers.pkl\"))\n",
    "joblib.dump(encoders, os.path.join(folder, \"encoders.pkl\"))\n",
    "joblib.dump(input_sizes, os.path.join(folder, \"input_sizes.pkl\"))\n",
    "joblib.dump(metrics, os.path.join(folder, \"metrics.pkl\"))\n",
    "print(\"All parameters saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Now change the used model in the source code of the web app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
